#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨åŸæœ‰æŠ¥å‘Šæ ¼å¼ç”ŸæˆSEOç»¼åˆæŠ¥å‘Š
ä¿æŒåŸæœ‰é¡¹ç›®çš„æŠ¥å‘Šç»“æ„å’Œæ ·å¼
"""
import sys
import os
import json
from datetime import datetime

# æ·»åŠ è·¯å¾„ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
script_dir = os.path.dirname(os.path.abspath(__file__))
platform_dir = os.path.join(script_dir, 'seo_unified_platform')
sys.path.insert(0, platform_dir)

from config import config
from core.quality_analyzer import QualityAnalyzer
from core.duplicate_analyzer import DuplicateAnalyzer
from core.seo_analyzer import SEOAnalyzer

# å¯¼å…¥åŸæœ‰æŠ¥å‘Šç”Ÿæˆå™¨
from services.generate_comprehensive_report import generate_html_report, merge_data

print("\n" + "="*80)
print("ğŸ“Š ç”ŸæˆSEOå†…å®¹è´¨é‡ç»¼åˆæŠ¥å‘Šï¼ˆåŸæœ‰æ ¼å¼ï¼‰")
print("="*80 + "\n")

# å‡†å¤‡æµ‹è¯•URL
urls = [
    "http://example.com",
    "http://example.org",
    "http://example.net",
    "https://httpbin.org/html",
    "https://www.python.org/",
    "https://github.com",
]

print(f"ğŸ“‹ åˆ†æURLåˆ—è¡¨ ({len(urls)}ä¸ª):")
for i, url in enumerate(urls):
    print(f"   {i+1}. {url}")

# åŠ è½½é…ç½®
app_config = config['default']

# åˆå§‹åŒ–åˆ†æå™¨
print("\nâš™ï¸  åˆå§‹åŒ–åˆ†æå™¨...")
quality_analyzer = QualityAnalyzer(app_config, qianfan_client=None)
duplicate_analyzer = DuplicateAnalyzer(app_config)
seo_analyzer = SEOAnalyzer(app_config, quality_analyzer, duplicate_analyzer)

# æ­¥éª¤1: æ‰§è¡Œè´¨é‡åˆ†æ
print("\nğŸ“ æ­¥éª¤1: æ‰§è¡Œè´¨é‡åˆ†æ...")
quality_results = quality_analyzer.batch_analyze(urls)
print(f"âœ… è´¨é‡åˆ†æå®Œæˆ")

# æ­¥éª¤2: æ‰§è¡Œé‡å¤æ£€æµ‹
print("\nğŸ” æ­¥éª¤2: æ‰§è¡Œé‡å¤æ£€æµ‹...")
duplicate_results = duplicate_analyzer.batch_analyze(urls)
print(f"âœ… é‡å¤æ£€æµ‹å®Œæˆ")

# æ­¥éª¤3: è½¬æ¢æ•°æ®æ ¼å¼ä¸ºåŸæŠ¥å‘Šæ ¼å¼
print("\nğŸ”„ æ­¥éª¤3: è½¬æ¢æ•°æ®æ ¼å¼...")

def convert_to_original_format(quality_results, duplicate_results):
    """å°†æ–°åˆ†æå™¨çš„ç»“æœè½¬æ¢ä¸ºåŸæŠ¥å‘Šç”Ÿæˆå™¨æœŸæœ›çš„æ ¼å¼"""

    # æ„å»ºç±»ä¼¼åŸé¡¹ç›®çš„SEOæ•°æ®ç»“æ„
    seo_data = {
        "url_info": {},
        "duplicate_rates": {},
        "paragraph_stats": {},
        "duplicate_paragraphs": {},
        "directory_groups": {},
        "config": {
            "duplicate_threshold": 15.0
        }
    }

    # æå–é‡å¤æ£€æµ‹æ•°æ®
    if duplicate_results and 'url_data' in duplicate_results:
        for url, data in duplicate_results['url_data'].items():
            if data.get('success'):
                seo_data["url_info"][url] = {
                    "publish_date": data.get('publish_date'),
                    "directory": data.get('directory', 'unknown')
                }
                seo_data["paragraph_stats"][url] = {
                    "total": data.get('total_paragraphs', 0),
                    "duplicate": 0  # ç¨åè®¡ç®—
                }

    # æå–é‡å¤ç‡
    if duplicate_results and 'similarities' in duplicate_results:
        seo_data["duplicate_rates"] = duplicate_results['similarities'].get('duplicate_rates', {})
        seo_data["duplicate_paragraphs"] = duplicate_results['similarities'].get('duplicate_paragraphs', {})

        # æ›´æ–°æ®µè½ç»Ÿè®¡
        for url, dup_paragraphs in seo_data["duplicate_paragraphs"].items():
            if url in seo_data["paragraph_stats"]:
                seo_data["paragraph_stats"][url]["duplicate"] = len(dup_paragraphs)

    # æ„å»ºè´¨é‡æ•°æ®ï¼ˆCSVæ ¼å¼æ¨¡æ‹Ÿï¼‰
    quality_data = {}
    for url, result in quality_results.items():
        if result.get('success'):
            analysis = result.get('analysis', {})
            quality_data[url] = {
                "has_implicit": analysis.get('has_implicit', False),
                "score": analysis.get('score', 0),
                "result": analysis.get('result', '')
            }

    return seo_data, quality_data

# è½¬æ¢æ•°æ®
seo_data, quality_data = convert_to_original_format(quality_results, duplicate_results)
print(f"âœ… æ•°æ®æ ¼å¼è½¬æ¢å®Œæˆ")

# æ­¥éª¤4: åˆå¹¶æ•°æ®
print("\nğŸ”— æ­¥éª¤4: åˆå¹¶æ•°æ®...")
merged_data = merge_data(seo_data, quality_data)
print(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ")

# æ˜¾ç¤ºç»Ÿè®¡
stats = merged_data.get('stats', {})
quality_stats = stats.get('quality_stats', {})
print(f"\nğŸ“Š åˆå¹¶åç»Ÿè®¡:")
print(f"   æ€»URLæ•°: {stats.get('total_urls', 0)}")
print(f"   ä¼˜: {quality_stats.get('excellent', 0)}")
print(f"   è‰¯: {quality_stats.get('good', 0)}")
print(f"   å·®: {quality_stats.get('fair', 0)}")
print(f"   æå·®: {quality_stats.get('poor', 0)}")

# æ­¥éª¤5: ç”ŸæˆHTMLæŠ¥å‘Š
print("\nğŸ“ æ­¥éª¤5: ç”ŸæˆHTMLæŠ¥å‘Š...")
# æŠ¥å‘Šç”Ÿæˆåˆ°content_analysis/reportsç›®å½•
output_dir = "/Users/tang/Desktop/python/content_analysis/reports"

report_dir = generate_html_report(merged_data, output_dir)

print(f"\nâœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ!")
print(f"   æŠ¥å‘Šç›®å½•: {report_dir}")

# æ‰“å¼€ç´¢å¼•é¡µé¢
import subprocess
index_path = os.path.join(report_dir, "index.html")
try:
    subprocess.run(['open', index_path])
    print(f"   âœ… æŠ¥å‘Šå·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€")
except Exception as e:
    print(f"   âš ï¸  è¯·æ‰‹åŠ¨æ‰“å¼€: {index_path}")

# åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
print(f"\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶:")
if os.path.exists(report_dir):
    for file in sorted(os.listdir(report_dir)):
        file_path = os.path.join(report_dir, file)
        if os.path.isfile(file_path):
            size = os.path.getsize(file_path)
            print(f"   â€¢ {file} ({size} bytes)")

print("\n" + "="*80)
print("âœ… åŸæœ‰æ ¼å¼çš„SEOç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
print("="*80 + "\n")
