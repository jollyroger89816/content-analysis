# -*- coding: utf-8 -*-
"""
SEOç»¼åˆåˆ†æžå™¨ - æ•´åˆè´¨é‡å’Œé‡å¤åº¦åˆ†æžçš„ç»¼åˆè¯„åˆ†ç³»ç»Ÿ
"""
import logging
from typing import Dict, List
from datetime import datetime

from .base_analyzer import BaseAnalyzer

logger = logging.getLogger(__name__)


class SEOAnalyzer(BaseAnalyzer):
    """SEOç»¼åˆåˆ†æžå™¨"""

    def __init__(self, config, quality_analyzer=None, duplicate_analyzer=None):
        """
        åˆå§‹åŒ–SEOç»¼åˆåˆ†æžå™¨

        Args:
            config: é…ç½®å¯¹è±¡
            quality_analyzer: è´¨é‡åˆ†æžå™¨å®žä¾‹
            duplicate_analyzer: é‡å¤åˆ†æžå™¨å®žä¾‹
        """
        super().__init__(config)
        self.quality_analyzer = quality_analyzer
        self.duplicate_analyzer = duplicate_analyzer

        # è¯„åˆ†æƒé‡
        self.implicit_language_weight = config.IMPLICIT_LANGUAGE_WEIGHT
        self.duplicate_content_weight = config.DUPLICATE_CONTENT_WEIGHT

    def analyze(self, url: str, quality_data: Dict = None, duplicate_data: Dict = None) -> Dict[str, any]:
        """
        åˆ†æžå•ä¸ªURLçš„ç»¼åˆSEOè¡¨çŽ°

        Args:
            url: è¦åˆ†æžçš„URL
            quality_data: è´¨é‡åˆ†æžæ•°æ®ï¼ˆå¯é€‰ï¼‰
            duplicate_data: é‡å¤åº¦åˆ†æžæ•°æ®ï¼ˆå¯é€‰ï¼‰

        Returns:
            ç»¼åˆåˆ†æžç»“æžœ
        """
        logger.info(f"ç»¼åˆåˆ†æžURL: {url}")

        # å¦‚æžœæ²¡æœ‰æä¾›æ•°æ®ï¼Œè°ƒç”¨ç›¸åº”çš„åˆ†æžå™¨
        if quality_data is None and self.quality_analyzer:
            quality_data = self.quality_analyzer.analyze(url)

        if duplicate_data is None and self.duplicate_analyzer:
            duplicate_data = self.duplicate_analyzer.analyze(url)

        # æå–æ•°æ®
        quality_info = self._extract_quality_info(quality_data)
        duplicate_info = self._extract_duplicate_info(duplicate_data)

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        seo_score = self._calculate_seo_score(quality_info, duplicate_info)

        # ç¡®å®šè´¨é‡ç­‰çº§
        quality_level = self._determine_quality_level(seo_score)

        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(quality_info, duplicate_info, seo_score)

        return {
            'url': url,
            'success': True,
            'publish_date': duplicate_info.get('publish_date'),
            'directory': duplicate_info.get('directory'),
            'quality_level': quality_level,
            'seo_score': seo_score,
            'quality_info': quality_info,
            'duplicate_info': duplicate_info,
            'recommendations': recommendations,
            'analyzed_at': datetime.now().isoformat()
        }

    def batch_analyze(self, urls: List[str], quality_results: Dict = None, duplicate_results: Dict = None) -> Dict:
        """
        æ‰¹é‡ç»¼åˆåˆ†æž

        Args:
            urls: URLåˆ—è¡¨
            quality_results: è´¨é‡åˆ†æžç»“æžœï¼ˆå¯é€‰ï¼‰
            duplicate_results: é‡å¤åº¦åˆ†æžç»“æžœï¼ˆå¯é€‰ï¼‰

        Returns:
            æ‰¹é‡ç»¼åˆåˆ†æžç»“æžœ
        """
        logger.info(f"å¼€å§‹æ‰¹é‡ç»¼åˆåˆ†æžï¼Œå…±{len(urls)}ä¸ªURL")

        results = {}

        for url in urls:
            quality_data = quality_results.get(url) if quality_results else None
            duplicate_data = duplicate_results.get(url, {}).get('url_data', {}).get(url) if duplicate_results else None

            try:
                result = self.analyze(url, quality_data, duplicate_data)
                results[url] = result
            except Exception as e:
                logger.error(f"ç»¼åˆåˆ†æžURL {url} æ—¶å‡ºé”™: {str(e)}")
                results[url] = {
                    'url': url,
                    'success': False,
                    'error': str(e)
                }

        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        stats = self._generate_batch_stats(results)

        return {
            'results': results,
            'stats': stats
        }

    def _extract_quality_info(self, quality_data: Dict) -> Dict:
        """
        æå–è´¨é‡ä¿¡æ¯

        Args:
            quality_data: è´¨é‡åˆ†æžæ•°æ®

        Returns:
            è´¨é‡ä¿¡æ¯å­—å…¸
        """
        if not quality_data or not quality_data.get('success'):
            return {
                'has_implicit': False,
                'implicit_score': 0,
                'implicit_level': 'æœªçŸ¥',
                'paragraphs_count': 0
            }

        analysis = quality_data.get('analysis', {})

        return {
            'has_implicit': analysis.get('has_implicit', False),
            'implicit_score': analysis.get('score', 0),
            'implicit_level': analysis.get('level', 'æœªçŸ¥'),
            'paragraphs_count': quality_data.get('paragraphs_count', 0)
        }

    def _extract_duplicate_info(self, duplicate_data: Dict) -> Dict:
        """
        æå–é‡å¤åº¦ä¿¡æ¯

        Args:
            duplicate_data: é‡å¤åº¦åˆ†æžæ•°æ®

        Returns:
            é‡å¤åº¦ä¿¡æ¯å­—å…¸
        """
        if not duplicate_data or not duplicate_data.get('success'):
            return {
                'duplicate_rate': 0.0,
                'duplicate_paragraphs': 0,
                'total_paragraphs': 0,
                'publish_date': None,
                'directory': 'unknown'
            }

        # å¦‚æžœæ˜¯æ‰¹é‡åˆ†æžç»“æžœï¼Œæå–URLç‰¹å®šæ•°æ®
        if 'similarities' in duplicate_data:
            # è¿™æ˜¯æ‰¹é‡ç»“æžœ
            url = duplicate_data.get('url', '')
            duplicate_rates = duplicate_data.get('similarities', {}).get('duplicate_rates', {})
            duplicate_paragraphs = duplicate_data.get('similarities', {}).get('duplicate_paragraphs', {})

            return {
                'duplicate_rate': duplicate_rates.get(url, 0.0),
                'duplicate_paragraphs': len(duplicate_paragraphs.get(url, [])),
                'total_paragraphs': duplicate_data.get('url_data', {}).get(url, {}).get('total_paragraphs', 0),
                'publish_date': duplicate_data.get('url_data', {}).get(url, {}).get('publish_date'),
                'directory': duplicate_data.get('url_data', {}).get(url, {}).get('directory')
            }
        else:
            # è¿™æ˜¯å•ä¸ªURLç»“æžœ
            return {
                'duplicate_rate': 0.0,  # å•ä¸ªåˆ†æžæ— æ³•è®¡ç®—é‡å¤çŽ‡
                'duplicate_paragraphs': 0,
                'total_paragraphs': duplicate_data.get('total_paragraphs', 0),
                'publish_date': duplicate_data.get('publish_date'),
                'directory': duplicate_data.get('directory', 'unknown')
            }

    def _calculate_seo_score(self, quality_info: Dict, duplicate_info: Dict) -> float:
        """
        è®¡ç®—ç»¼åˆSEOè¯„åˆ†

        è¯„åˆ†å…¬å¼:
        - é‡å¤å†…å®¹è¯„åˆ† = 100 - é‡å¤çŽ‡
        - æš—ç¤ºæ€§è¯­è¨€è¯„åˆ† = 100 - (æš—ç¤ºåˆ†æ•° * 10)
        - ç»¼åˆè¯„åˆ† = é‡å¤å†…å®¹æƒé‡ * é‡å¤è¯„åˆ† + æš—ç¤ºè¯­è¨€æƒé‡ * æš—ç¤ºè¯„åˆ†

        Args:
            quality_info: è´¨é‡ä¿¡æ¯
            duplicate_info: é‡å¤åº¦ä¿¡æ¯

        Returns:
            ç»¼åˆSEOè¯„åˆ† (0-100)
        """
        # 1. é‡å¤å†…å®¹è¯„åˆ† (0-100ï¼Œ100æœ€å¥½)
        duplicate_rate = duplicate_info.get('duplicate_rate', 0.0)
        duplicate_score = max(0, 100 - duplicate_rate)

        # 2. æš—ç¤ºæ€§è¯­è¨€è¯„åˆ† (0-100ï¼Œ100æœ€å¥½)
        implicit_score = quality_info.get('implicit_score', 0)
        normalized_implicit_score = max(0, 100 - implicit_score * 10)

        # 3. åŠ æƒå¹³å‡
        seo_score = (
            self.duplicate_content_weight * duplicate_score +
            self.implicit_language_weight * normalized_implicit_score
        )

        return round(seo_score, 2)

    def _determine_quality_level(self, seo_score: float) -> str:
        """
        æ ¹æ®SEOè¯„åˆ†ç¡®å®šè´¨é‡ç­‰çº§

        Args:
            seo_score: SEOç»¼åˆè¯„åˆ†

        Returns:
            è´¨é‡ç­‰çº§ (ä¼˜/è‰¯/å·®/æžå·®)
        """
        if seo_score >= 85:
            return 'ä¼˜'
        elif seo_score >= 70:
            return 'è‰¯'
        elif seo_score >= 50:
            return 'å·®'
        else:
            return 'æžå·®'

    def _generate_recommendations(self, quality_info: Dict, duplicate_info: Dict, seo_score: float) -> List[str]:
        """
        ç”Ÿæˆä¼˜åŒ–å»ºè®®

        Args:
            quality_info: è´¨é‡ä¿¡æ¯
            duplicate_info: é‡å¤åº¦ä¿¡æ¯
            seo_score: SEOè¯„åˆ†

        Returns:
            å»ºè®®åˆ—è¡¨
        """
        recommendations = []

        # åŸºäºŽè¯„åˆ†ç­‰çº§çš„å»ºè®®
        if seo_score >= 85:
            recommendations.append("âœ… é¡µé¢è´¨é‡ä¼˜ç§€ï¼Œç»§ç»­ä¿æŒ")
        elif seo_score >= 70:
            recommendations.append("âš ï¸ é¡µé¢è´¨é‡è‰¯å¥½ï¼Œæœ‰ä¼˜åŒ–ç©ºé—´")
        else:
            recommendations.append("âŒ é¡µé¢è´¨é‡éœ€è¦ä¼˜åŒ–")

        # åŸºäºŽæš—ç¤ºæ€§è¯­è¨€çš„å»ºè®®
        if quality_info.get('has_implicit'):
            implicit_level = quality_info.get('implicit_level', '')
            implicit_score = quality_info.get('implicit_score', 0)

            if implicit_level == 'å¼ºçƒˆ' or implicit_score >= 7:
                recommendations.append("ðŸ”´ æ£€æµ‹åˆ°å¼ºçƒˆæš—ç¤ºæ€§è¯­è¨€ï¼Œå»ºè®®ä¿®æ”¹ä¸ºæ˜Žç¡®è¡¨è¿°")
            elif implicit_level == 'ä¸­ç­‰' or implicit_score >= 5:
                recommendations.append("ðŸŸ¡ æ£€æµ‹åˆ°ä¸­ç­‰ç¨‹åº¦æš—ç¤ºæ€§è¯­è¨€ï¼Œå»ºè®®ä¼˜åŒ–")
            elif implicit_level == 'è½»å¾®' or implicit_score >= 3:
                recommendations.append("ðŸŸ¢ æ£€æµ‹åˆ°è½»å¾®æš—ç¤ºæ€§è¯­è¨€ï¼Œå¯ä»¥é€‚å½“æ”¹è¿›")
        else:
            recommendations.append("âœ… æœªæ£€æµ‹åˆ°æš—ç¤ºæ€§è¯­è¨€ï¼Œè¡¨è¿°æ˜Žç¡®")

        # åŸºäºŽé‡å¤åº¦çš„å»ºè®®
        duplicate_rate = duplicate_info.get('duplicate_rate', 0.0)
        duplicate_threshold = self.config.DUPLICATE_THRESHOLD

        if duplicate_rate > duplicate_threshold * 2:
            recommendations.append(f"ðŸ”´ å†…å®¹é‡å¤çŽ‡è¿‡é«˜({duplicate_rate:.1f}%)ï¼Œå¼ºçƒˆå»ºè®®é‡å†™")
        elif duplicate_rate > duplicate_threshold:
            recommendations.append(f"ðŸŸ¡ å†…å®¹é‡å¤çŽ‡è¾ƒé«˜({duplicate_rate:.1f}%)ï¼Œå»ºè®®ä¿®æ”¹")
        elif duplicate_rate > 0:
            recommendations.append(f"âœ… å†…å®¹é‡å¤çŽ‡åœ¨å¯æŽ¥å—èŒƒå›´({duplicate_rate:.1f}%)")
        else:
            recommendations.append("âœ… å†…å®¹åŽŸåˆ›æ€§è‰¯å¥½")

        return recommendations

    def _generate_batch_stats(self, results: Dict) -> Dict:
        """
        ç”Ÿæˆæ‰¹é‡åˆ†æžç»Ÿè®¡ä¿¡æ¯

        Args:
            results: åˆ†æžç»“æžœå­—å…¸

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        total_urls = len(results)
        successful = sum(1 for r in results.values() if r.get('success'))

        quality_levels = {}
        avg_score = 0
        has_implicit_count = 0
        high_duplicate_count = 0

        for result in results.values():
            if result.get('success'):
                # è´¨é‡ç­‰çº§ç»Ÿè®¡
                level = result.get('quality_level', 'æœªçŸ¥')
                quality_levels[level] = quality_levels.get(level, 0) + 1

                # å¹³å‡è¯„åˆ†
                avg_score += result.get('seo_score', 0)

                # æš—ç¤ºæ€§è¯­è¨€ç»Ÿè®¡
                if result.get('quality_info', {}).get('has_implicit'):
                    has_implicit_count += 1

                # é«˜é‡å¤åº¦ç»Ÿè®¡
                if result.get('duplicate_info', {}).get('duplicate_rate', 0) > self.config.DUPLICATE_THRESHOLD:
                    high_duplicate_count += 1

        if successful > 0:
            avg_score = round(avg_score / successful, 2)

        return {
            'total_urls': total_urls,
            'successful_analyses': successful,
            'failed_analyses': total_urls - successful,
            'quality_distribution': quality_levels,
            'average_seo_score': avg_score,
            'has_implicit_count': has_implicit_count,
            'high_duplicate_count': high_duplicate_count
        }
