#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„SEOåˆ†ææµç¨‹ - æ¨¡æ‹ŸåŸæœ‰é¡¹ç›®çš„å·¥ä½œæ–¹å¼
1. åŒæ—¶å¯åŠ¨è´¨é‡æ£€æµ‹å’Œé‡å¤æ£€æµ‹ï¼ˆå¹¶è¡Œï¼‰
2. ç­‰å¾…ä¸¤ä¸ªè„šæœ¬éƒ½å®Œæˆ
3. åˆå¹¶ç»“æœç”Ÿæˆç»¼åˆæŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    python full_seo_analysis.py [url_file]

ç¤ºä¾‹:
    python full_seo_analysis.py urls.txt
"""
import sys
import os
import json
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# æ·»åŠ è·¯å¾„ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
script_dir = os.path.dirname(os.path.abspath(__file__))
platform_dir = os.path.join(script_dir, 'seo_unified_platform')
sys.path.insert(0, platform_dir)

from config import config
from core.quality_analyzer import QualityAnalyzer
from core.duplicate_analyzer import DuplicateAnalyzer
from services.generate_comprehensive_report import generate_html_report, merge_data

print("\n" + "="*80)
print("ğŸš€ å®Œæ•´SEOåˆ†ææµç¨‹ï¼ˆåŸæœ‰å·¥ä½œæ–¹å¼ï¼‰")
print("="*80)

# åŠ è½½URLåˆ—è¡¨ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼‰
if len(sys.argv) > 1:
    url_file = sys.argv[1]
elif os.path.exists("/tmp/test_urls_clean.txt"):
    url_file = "/tmp/test_urls_clean.txt"
else:
    print("\nâŒ é”™è¯¯: è¯·æŒ‡å®šURLæ–‡ä»¶")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("  python full_seo_analysis.py <url_file>")
    print("\nç¤ºä¾‹:")
    print("  python full_seo_analysis.py urls.txt")
    sys.exit(1)

print(f"\nğŸ“‚ ä»æ–‡ä»¶åŠ è½½URL: {url_file}")

with open(url_file, 'r', encoding='utf-8') as f:
    urls = [line.strip() for line in f if line.strip()]

print(f"âœ… åŠ è½½äº† {len(urls)} ä¸ªURL")

# é…ç½®
app_config = config['default']
# æŠ¥å‘Šç”Ÿæˆåˆ°content_analysis/reportsç›®å½•
output_dir = "/Users/tang/Desktop/python/content_analysis/reports"

# åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜ä¸­é—´ç»“æœ
temp_dir = os.path.join(output_dir, "temp_analysis")
os.makedirs(temp_dir, exist_ok=True)

quality_json_path = os.path.join(temp_dir, "quality_results.json")
duplicate_json_path = os.path.join(temp_dir, "duplicate_results.json")

# ç”¨äºå­˜å‚¨ç»“æœ
results = {
    'quality': None,
    'duplicate': None,
    'quality_time': 0,
    'duplicate_time': 0
}
lock = threading.Lock()

def run_quality_analysis(urls):
    """è¿è¡Œè´¨é‡åˆ†æ"""
    print("\n" + "-"*80)
    print("ğŸ“ ã€ä»»åŠ¡1ã€‘å¯åŠ¨è´¨é‡æ£€æµ‹è„šæœ¬...")
    print("-"*80)

    start_time = time.time()

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = QualityAnalyzer(app_config, qianfan_client=None)

    # æ‰§è¡Œåˆ†æ
    quality_results = analyzer.batch_analyze(urls)

    elapsed = time.time() - start_time

    # ä¿å­˜ç»“æœåˆ°JSON
    with open(quality_json_path, 'w', encoding='utf-8') as f:
        json.dump(quality_results, f, ensure_ascii=False, indent=2)

    with lock:
        results['quality'] = quality_results
        results['quality_time'] = elapsed

    successful = sum(1 for r in quality_results.values() if r.get('success'))

    print(f"\nâœ… ã€ä»»åŠ¡1å®Œæˆã€‘è´¨é‡æ£€æµ‹è„šæœ¬æ‰§è¡Œå®Œæˆ")
    print(f"   åˆ†æURLæ•°: {len(quality_results)}")
    print(f"   æˆåŠŸ: {successful}")
    print(f"   å¤±è´¥: {len(quality_results) - successful}")
    print(f"   è€—æ—¶: {elapsed:.2f}ç§’")
    print(f"   ç»“æœå·²ä¿å­˜åˆ°: {quality_json_path}")

    return quality_results

def run_duplicate_analysis(urls):
    """è¿è¡Œé‡å¤æ£€æµ‹"""
    print("\n" + "-"*80)
    print("ğŸ” ã€ä»»åŠ¡2ã€‘å¯åŠ¨é‡å¤æ£€æµ‹è„šæœ¬...")
    print("-"*80)

    start_time = time.time()

    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = DuplicateAnalyzer(app_config)

    # æ‰§è¡Œåˆ†æ
    duplicate_results = analyzer.batch_analyze(urls)

    elapsed = time.time() - start_time

    # ä¿å­˜ç»“æœåˆ°JSON
    with open(duplicate_json_path, 'w', encoding='utf-8') as f:
        json.dump(duplicate_results, f, ensure_ascii=False, indent=2)

    with lock:
        results['duplicate'] = duplicate_results
        results['duplicate_time'] = elapsed

    if duplicate_results and 'url_data' in duplicate_results:
        successful = sum(1 for r in duplicate_results['url_data'].values() if r.get('success'))
        stats = duplicate_results.get('stats', {})

        print(f"\nâœ… ã€ä»»åŠ¡2å®Œæˆã€‘é‡å¤æ£€æµ‹è„šæœ¬æ‰§è¡Œå®Œæˆ")
        print(f"   åˆ†æURLæ•°: {len(duplicate_results['url_data'])}")
        print(f"   æˆåŠŸ: {successful}")
        print(f"   å¤±è´¥: {len(duplicate_results['url_data']) - successful}")
        print(f"   é«˜é‡å¤URL: {stats.get('high_duplicate_count', 0)}")
        print(f"   å¹³å‡é‡å¤ç‡: {stats.get('avg_duplicate_rate', 0)}%")
        print(f"   è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"   ç»“æœå·²ä¿å­˜åˆ°: {duplicate_json_path}")

    return duplicate_results

# æ­¥éª¤1: å¹¶è¡Œå¯åŠ¨ä¸¤ä¸ªåˆ†æè„šæœ¬
print("\n" + "="*80)
print("ğŸ”„ æ­¥éª¤1: åŒæ—¶å¯åŠ¨è´¨é‡æ£€æµ‹å’Œé‡å¤æ£€æµ‹è„šæœ¬ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰")
print("="*80)

overall_start = time.time()

# ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
with ThreadPoolExecutor(max_workers=2) as executor:
    # æäº¤ä¸¤ä¸ªä»»åŠ¡
    future_quality = executor.submit(run_quality_analysis, urls)
    future_duplicate = executor.submit(run_duplicate_analysis, urls)

    # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡éƒ½å®Œæˆ
    print("\nâ³ ç­‰å¾…ä¸¤ä¸ªè„šæœ¬å®Œæˆ...")
    futures = {future_quality: 'quality', future_duplicate: 'duplicate'}

    for future in as_completed(futures):
        task_name = futures[future]
        try:
            future.result()
        except Exception as e:
            print(f"\nâŒ {task_name} ä»»åŠ¡å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()

parallel_time = time.time() - overall_start

# æ­¥éª¤2: ç­‰å¾…å¹¶ç¡®è®¤ä¸¤ä¸ªè„šæœ¬éƒ½å®Œæˆ
print("\n" + "="*80)
print("â¸ï¸  æ­¥éª¤2: ä¸¤ä¸ªè„šæœ¬éƒ½å·²æ‰§è¡Œå®Œæˆ")
print("="*80)

print(f"\nğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
print(f"   è´¨é‡æ£€æµ‹è€—æ—¶: {results['quality_time']:.2f}ç§’")
print(f"   é‡å¤æ£€æµ‹è€—æ—¶: {results['duplicate_time']:.2f}ç§’")
print(f"   æ€»è€—æ—¶ï¼ˆå¹¶è¡Œï¼‰: {parallel_time:.2f}ç§’")
print(f"   èŠ‚çœæ—¶é—´: {results['quality_time'] + results['duplicate_time'] - parallel_time:.2f}ç§’")

# æ­¥éª¤3: åˆå¹¶æ•°æ®
print("\n" + "="*80)
print("ğŸ”— æ­¥éª¤3: åˆå¹¶ä¸¤ä¸ªè„šæœ¬çš„è¾“å‡ºæ•°æ®")
print("="*80)

def convert_to_original_format(quality_results, duplicate_results):
    """å°†æ–°åˆ†æå™¨çš„ç»“æœè½¬æ¢ä¸ºåŸæŠ¥å‘Šç”Ÿæˆå™¨æœŸæœ›çš„æ ¼å¼"""

    # æ„å»ºç±»ä¼¼åŸé¡¹ç›®çš„SEOæ•°æ®ç»“æ„
    seo_data = {
        "url_info": {},
        "duplicate_rates": {},
        "paragraph_stats": {},
        "duplicate_paragraphs": {},
        "directory_groups": {},
        "config": {
            "duplicate_threshold": 15.0
        }
    }

    # æå–é‡å¤æ£€æµ‹æ•°æ®
    if duplicate_results and 'url_data' in duplicate_results:
        for url, data in duplicate_results['url_data'].items():
            if data.get('success'):
                seo_data["url_info"][url] = {
                    "publish_date": data.get('publish_date'),
                    "directory": data.get('directory', 'unknown')
                }
                seo_data["paragraph_stats"][url] = {
                    "total": data.get('total_paragraphs', 0),
                    "duplicate": 0
                }

    # æå–é‡å¤ç‡
    if duplicate_results and 'similarities' in duplicate_results:
        seo_data["duplicate_rates"] = duplicate_results['similarities'].get('duplicate_rates', {})
        seo_data["duplicate_paragraphs"] = duplicate_results['similarities'].get('duplicate_paragraphs', {})

        # æ›´æ–°æ®µè½ç»Ÿè®¡
        for url, dup_paragraphs in seo_data["duplicate_paragraphs"].items():
            if url in seo_data["paragraph_stats"]:
                seo_data["paragraph_stats"][url]["duplicate"] = len(dup_paragraphs)

    # æ„å»ºè´¨é‡æ•°æ®
    quality_data = {}
    for url, result in quality_results.items():
        if result.get('success'):
            analysis = result.get('analysis', {})
            quality_data[url] = {
                "has_implicit": analysis.get('has_implicit', False),
                "score": analysis.get('score', 0),
                "result": analysis.get('result', '')
            }

    return seo_data, quality_data

# è½¬æ¢æ•°æ®æ ¼å¼
seo_data, quality_data = convert_to_original_format(results['quality'], results['duplicate'])

print(f"âœ… æ•°æ®æ ¼å¼è½¬æ¢å®Œæˆ")

# ä½¿ç”¨åŸæœ‰çš„merge_dataå‡½æ•°
merged_data = merge_data(seo_data, quality_data)

# æ˜¾ç¤ºç»Ÿè®¡
stats = merged_data.get('stats', {})
quality_stats = stats.get('quality_stats', {})

print(f"\nğŸ“Š åˆå¹¶åç»Ÿè®¡:")
print(f"   æ€»URLæ•°: {stats.get('total_urls', 0)}")
print(f"   ä¼˜: {quality_stats.get('excellent', 0)}")
print(f"   è‰¯: {quality_stats.get('good', 0)}")
print(f"   å·®: {quality_stats.get('fair', 0)}")
print(f"   æå·®: {quality_stats.get('poor', 0)}")
print(f"   é«˜é‡å¤URL: {stats.get('high_duplicate', 0)}")
print(f"   æš—ç¤ºè¯­è¨€URL: {stats.get('has_implicit', 0)}")
print(f"   åŒé‡é—®é¢˜URL: {stats.get('both_issues', 0)}")

# æ­¥éª¤4: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
print("\n" + "="*80)
print("ğŸ“ æ­¥éª¤4: ç”ŸæˆSEOå†…å®¹è´¨é‡ç»¼åˆæŠ¥å‘Š")
print("="*80)

report_start = time.time()

report_dir = generate_html_report(merged_data, output_dir)

report_time = time.time() - report_start

total_time = time.time() - overall_start

print(f"\nâœ… ç»¼åˆæŠ¥å‘Šç”ŸæˆæˆåŠŸ!")
print(f"   æŠ¥å‘Šç›®å½•: {report_dir}")
print(f"   æŠ¥å‘Šç”Ÿæˆè€—æ—¶: {report_time:.2f}ç§’")

# æ‰“å¼€æŠ¥å‘Š
import subprocess
index_path = os.path.join(report_dir, "index.html")
try:
    subprocess.run(['open', index_path])
    print(f"   âœ… æŠ¥å‘Šå·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€")
except Exception as e:
    print(f"   è¯·æ‰‹åŠ¨æ‰“å¼€: {index_path}")

# æ€»ç»“
print("\n" + "="*80)
print("ğŸ‰ å®Œæ•´SEOåˆ†ææµç¨‹æ€»ç»“")
print("="*80)

print(f"\nâ±ï¸  æ—¶é—´ç»Ÿè®¡:")
print(f"   è´¨é‡æ£€æµ‹: {results['quality_time']:.2f}ç§’")
print(f"   é‡å¤æ£€æµ‹: {results['duplicate_time']:.2f}ç§’")
print(f"   å¹¶è¡Œæ‰§è¡Œ: {parallel_time:.2f}ç§’")
print(f"   æŠ¥å‘Šç”Ÿæˆ: {report_time:.2f}ç§’")
print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")

print(f"\nğŸ“Š åˆ†æç»“æœ:")
print(f"   æ€»URLæ•°: {len(urls)}")
print(f"   è´¨é‡åˆ†å¸ƒ: ä¼˜{quality_stats.get('excellent', 0)} / è‰¯{quality_stats.get('good', 0)} / å·®{quality_stats.get('fair', 0)} / æå·®{quality_stats.get('poor', 0)}")

print(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
print(f"   æŠ¥å‘Šç›®å½•: {report_dir}")
print(f"   è´¨é‡ç»“æœ: {quality_json_path}")
print(f"   é‡å¤æ£€æµ‹ç»“æœ: {duplicate_json_path}")

print(f"\nâœ… åˆ†æå®Œæˆï¼è¿™æ˜¯ä¸åŸæœ‰é¡¹ç›®å®Œå…¨ä¸€è‡´çš„å·¥ä½œæµç¨‹ï¼")
print("="*80 + "\n")
